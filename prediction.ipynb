{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can you predict the Tide?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "Participants will have to forecast the sea surges in two western European coastal cities.\n",
    "\n",
    "We place ourselves in a forecast setup: knowing the surge values and the sea-level pressure field in the last 5 days, we want to predict the surge values in the next five days. It is hence a time series prediction problem. The signals we consider are:\n",
    "\n",
    "- the surge, which is a function of the time.\n",
    "- the sea-level pressure, which is a function of the time, the latitude and the longitude.\n",
    "\n",
    "The score $l(\\hat{y},y)$ we use to measure the quality of the prediction $\\hat{y}$​ compared to the true values $y$ is a weighted version of the mean square error (MSE). The weights depend linearly on the forecast time, with a bigger weight for the first forecast time and a lower weight for the last forecast time. The prediction for the two cities are computed independently, and the final loss is their sum:\n",
    "\n",
    "```python\n",
    "def surge_prediction_metric(y_true, y_pred):\n",
    "    w = np.linspace(1, 0.1, 10)[np.newaxis]\n",
    "    surge1_cols = [\n",
    "        'surge1_t0', 'surge1_t1', 'surge1_t2', 'surge1_t3', 'surge1_t4',\n",
    "        'surge1_t5', 'surge1_t6', 'surge1_t7', 'surge1_t8', 'surge1_t9' ]\n",
    "    surge2_cols = [\n",
    "        'surge2_t0', 'surge2_t1', 'surge2_t2', 'surge2_t3', 'surge2_t4',\n",
    "        'surge2_t5', 'surge2_t6', 'surge2_t7', 'surge2_t8', 'surge2_t9' ]\n",
    "    surge1_score = (w * (y_true[surge1_cols].values - y_pred[surge1_cols].values)**2).mean()\n",
    "    surge2_score = (w * (y_true[surge2_cols].values - y_pred[surge2_cols].values)**2).mean()\n",
    "\n",
    "    return surge1_score + surge2_score\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the surge values are normalized (zero mean and standard deviation 1), $1-l$ can be seen as a percentage of explained variance. With a trivial zero prediction of all values, the score is $l \\approx 1$ , meaning that we explain 0 % of the variance. A score bigger than one is hence worse that the zero prediction and can be considered as \"bad\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data description\n",
    "\n",
    "The training set contains 5599 entries, and the test set contains 509 entries. Each entry represents approximately 5 days of measurements of the pressure and the tide, and the times at which they were done.\n",
    "\n",
    "**Times** are given in the GMT convention. In the GMT convention, the time is expressed as the number of seconds elapsed since January 1st, 1970; they can be converted to the usual Gregorian time with `time.gmtime()`. Note that in our dataset, some times are negative: indeed, the first measurements date back to the 1950s. For instance, the very first sea-level pressure field is given at $t=−631108800.0$ which corresponds to 1950, January 1st at approximately 21h.\n",
    "\n",
    "**Sea-Level** Pressure fields (SLP) are given every three hours, so there are 40 fields for every observation. They cover the Atlantic front of western Europe and Iceland, as shown on the following map:\n",
    "\n",
    "![SLP map](images/slp.png)\n",
    "\n",
    "Sea surges are measured for each high tide, i.e. every 12 hours approximately. We measure the sea surge at two different locations, which are two anonymous European costal cities. Consequently, each entry contains `2×102×10` values to predict. There are in total $20×509=10180$ values to predict for the test. Note that the surge values in cities 1 and 2 have been normalized, such that they have 0 mean and standard deviation 1. The true means and std of the surge are of the order of 10cm and 20cm respectively.\n",
    "\n",
    "The **.npz format**: practically, the input X is encoded in the numpy .npz format and consists of:\n",
    "\n",
    "- id_sequence: the ids of the sequence\n",
    "- t_slp: the 40 GMT times at which the sea-level pressure (SLP) fields are given.\n",
    "- slp: the 40 sea-level pressure (SLP) fields, encoded in images of size (41, 41).\n",
    "- t_surge1_input: the 10 GMT times at which the surge heights are given in city 1.\n",
    "- surge1_input: the given surge heights in city 1.\n",
    "- t_surge2_input: the 10 GMT times at which the surge heights are given in city 2.\n",
    "- surge2_input: the given surge heights in city 2.\n",
    "- t_surge1_output: the 10 GMT times at which we must predict surge heights in city 1.\n",
    "- t_surge2_output: the 10 GMT times at which we must predict surge heights in city 2.\n",
    "\n",
    "To access for example the training slp, one can use the following:\n",
    "\n",
    "```python\n",
    "X_train = np.load('X_train_surge.npz')\n",
    "slp = X_train['slp']\n",
    "```\n",
    "\n",
    "The output Y is encoded in a csv file with the columns:\n",
    "\n",
    "\n",
    "- id_sequence: the ids of the sequence\n",
    "- surge1_t{0...9}: the correct surge height in city 1 at time 0 to 9\n",
    "- surge2_t{0...9}: the correct surge height in city 2 at time 0 to 9\n",
    "\n",
    "**Submission format** We provided a random submission example. The submission index must be X_test['id_sequence'] and the columns must match those of Y_train_surge.csv: see the notebook in the supplementary files for another submission example. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark description\n",
    "\n",
    "Analog methods in meteorology are the equivalent of nearest-neighbors predictions in machine learning. It consists in finding a day in the past when the weather scenario looked very similar to the actual weather scenario (an analog scenario). The forecaster would predict that the weather in this forecast will behave the same as it did in the past. Physicists appreciate these methods for their interpretability, as the prediction is explained by a real past situation that they can look at. They can also a posteriori analyze the prediction errors given the difference between the actual scenario and the analog found in the past.\n",
    "\n",
    "The benchmark we propose here is an analog method. We use the standard L2 metric and look for the closest k=40k=40 scenarios at time t and t - 24h using a K-nearest neighbor search. We then average over these scenarios to get the benchmark. It yields a score of 0.77 on the public test data, meaning that it explained around 23 % of the variance.\n",
    "\n",
    "We provide in the supplementary files a Jupyter Notebook that implements this benchmark."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(os.path.join('assets', 'X_train_surge_new.npz'))\n",
    "Y_train = pd.read_csv(os.path.join('assets', 'Y_train_surge.csv'))\n",
    "X_test = np.load(os.path.join('assets', 'X_test_surge_new.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5599, 40, 41, 41) (5599, 21)\n"
     ]
    }
   ],
   "source": [
    "s0: tuple[int, ...] = X_train['slp'].shape  # (5599, 40, 41, 41)\n",
    "s1: tuple[int, int] = Y_train.shape         # (5599, 21)\n",
    "print(s0, s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a gif of all 40 images of the first sample\n",
    "# from https://stackoverflow.com/a/56244531/1164295\n",
    "def create_gif(images, filename, duration=0.1):\n",
    "  import imageio\n",
    "  images = [imageio.imread(image) for image in images]\n",
    "  imageio.mimsave(filename, images, duration=duration)\n",
    "\n",
    "\n",
    "# try create assets/images folder\n",
    "try:\n",
    "  os.mkdir('assets/images')\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "if not os.path.exists(os.path.join('assets', 'images', 'surge_sample.gif')):\n",
    "\n",
    "  images = []\n",
    "  for i in range(s0[1]):\n",
    "    data = X_train['slp'][0, i, :, :]\n",
    "    plt.imsave(os.path.join('assets', 'images', f'surge_sample_{i}.png'), data, cmap='jet')\n",
    "    images.append(os.path.join('assets', 'images', f'surge_sample_{i}.png'))\n",
    "\n",
    "  create_gif(images, os.path.join('assets', 'images', f'surge_sample.gif'), duration=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"assets\\images\\surge_sample.gif\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display the gif\n",
    "from IPython.display import Image\n",
    "Image(url=os.path.join('assets', 'images', f'surge_sample.gif'), width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "df: np.ndarray = (X_train['slp'] - X_train['slp'].mean()) / X_train['slp'].std()\n",
    "df = df.reshape((s0[0], s0[1], s0[2] * s0[3]))\n",
    "del X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create time series model\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.LSTM(128, input_shape=(s0[1], s0[2] * s0[3])),\n",
    "  tf.keras.layers.Dense(21)\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "175/175 [==============================] - 5s 11ms/step - loss: 491800.2812\n",
      "Epoch 2/10\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 485070.4062\n",
      "Epoch 3/10\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 479047.2500\n",
      "Epoch 4/10\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 473158.1250\n",
      "Epoch 5/10\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 467317.7812\n",
      "Epoch 6/10\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 461603.6562\n",
      "Epoch 7/10\n",
      "175/175 [==============================] - 2s 11ms/step - loss: 455997.4375\n",
      "Epoch 8/10\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 450461.0625\n",
      "Epoch 9/10\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 444996.6875\n",
      "Epoch 10/10\n",
      "175/175 [==============================] - 2s 10ms/step - loss: 439600.0000\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "history = model.fit(df, Y_train.values, epochs=10, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
